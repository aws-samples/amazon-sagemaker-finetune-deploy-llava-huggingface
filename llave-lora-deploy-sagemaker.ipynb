{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/generation_config.json to lora-model/generation_config.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/config.json to lora-model/config.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/special_tokens_map.json to lora-model/special_tokens_map.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/tokenizer_config.json to lora-model/tokenizer_config.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/model.safetensors.index.json to lora-model/model.safetensors.index.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/tokenizer.model to lora-model/tokenizer.model\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/trainer_state.json to lora-model/trainer_state.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/training_args.bin to lora-model/training_args.bin\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/model-00003-of-00003.safetensors to lora-model/model-00003-of-00003.safetensors\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/model-00001-of-00003.safetensors to lora-model/model-00001-of-00003.safetensors\n",
      "download: s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/model-00002-of-00003.safetensors to lora-model/model-00002-of-00003.safetensors\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/debug-output/training_job_end.ts to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/debug-output/training_job_end.ts\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/checkpoints/README.md to lora-model/adapter_model/checkpoints/README.md\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/checkpoints/config.json to lora-model/adapter_model/checkpoints/config.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/framework/training_job_end.ts to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/framework/training_job_end.ts\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/checkpoints/trainer_state.json to lora-model/adapter_model/checkpoints/trainer_state.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/checkpoints/adapter_config.json to lora-model/adapter_model/checkpoints/adapter_config.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155300.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155300.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155360.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155360.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155480.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155480.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155420.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155420.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155540.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155540.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155600.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155600.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155660.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155660.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155780.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155780.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155720.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155720.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155960.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155960.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155840.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155840.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155900.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708155900.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/training_job_end.ts to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/training_job_end.ts\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708156140.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708156140.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708156200.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708156200.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708156080.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708156080.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708156020.algo-1.json to lora-model/adapter_model/output/llava-v15-7b-task-lora-2024-02-17-07-33-50/profiler-output/system/incremental/2024021707/1708156020.algo-1.json\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/source/sourcedir.tar.gz to lora-model/adapter_model/source/sourcedir.tar.gz\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/checkpoints/non_lora_trainables.bin to lora-model/adapter_model/checkpoints/non_lora_trainables.bin\n",
      "download: s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/checkpoints/adapter_model.safetensors to lora-model/adapter_model/checkpoints/adapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p lora-model/code\n",
    "!mkdir -p lora-model/adapter_model\n",
    "\n",
    "!aws s3 cp s3://llava-ue1/llava-v15-7b-task-full-2024-02-23-01-55-53/checkpoints/ lora-model/ --recursive\n",
    "!aws s3 cp s3://llava-ue1/llava-v15-7b-task-lora-2024-02-17-07-33-50/checkpoints/ lora-model/adapter_model --recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf llava-lora.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lora-model/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lora-model/code/inference.py\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from llava.model import LlavaLlamaForCausalLM\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, KeywordsStoppingCriteria\n",
    "\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    kwargs = {\"device_map\": \"auto\"}\n",
    "    kwargs[\"torch_dtype\"] = torch.float16\n",
    "    adapter_path = os.path.join(model_dir, \"adapter_model\")\n",
    "    \n",
    "    print(\"loading lora config\")\n",
    "    from llava.model.language_model.llava_llama import LlavaConfig\n",
    "    lora_cfg_pretrained = LlavaConfig.from_pretrained(adapter_path)\n",
    "    print(\"loading model\")\n",
    "    model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "        model_dir, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs\n",
    "    )\n",
    "    token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n",
    "    if model.lm_head.weight.shape[0] != token_num:\n",
    "        model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n",
    "        model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n",
    "\n",
    "    if os.path.exists(os.path.join(adapter_path, 'non_lora_trainables.bin')):\n",
    "        non_lora_trainables = torch.load(os.path.join(adapter_path, 'non_lora_trainables.bin'), map_location='cpu')\n",
    "\n",
    "    non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "    if any(k.startswith('model.model.') for k in non_lora_trainables):\n",
    "        non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "    model.load_state_dict(non_lora_trainables, strict=False)\n",
    "\n",
    "    from peft import PeftModel\n",
    "    print('Loading LoRA weights...')\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    print('Merging LoRA weights...')\n",
    "    model = model.merge_and_unload()\n",
    "    print('Model is loaded...')\n",
    "\n",
    "    print(\"loading tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
    "\n",
    "    vision_tower = model.get_vision_tower()\n",
    "    if not vision_tower.is_loaded:\n",
    "        vision_tower.load_model()\n",
    "    vision_tower.to(device=\"cuda\", dtype=torch.float16)\n",
    "    image_processor = vision_tower.image_processor\n",
    "    return model, tokenizer, image_processor\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    # unpack model and tokenizer\n",
    "    model, tokenizer, image_processor = model_and_tokenizer\n",
    "\n",
    "    # get prompt & parameters\n",
    "    image_file = data.pop(\"image\", data)\n",
    "    raw_prompt = data.pop(\"question\", data)\n",
    "\n",
    "    max_new_tokens = data.pop(\"max_new_tokens\", 1024)\n",
    "    temperature = data.pop(\"temperature\", 0.2)\n",
    "    conv_mode = data.pop(\"conv_mode\", \"llava_v1\")\n",
    "\n",
    "    if conv_mode == \"raw\":\n",
    "        # use raw_prompt as prompt\n",
    "        prompt = raw_prompt\n",
    "        stop_str = \"###\"\n",
    "    else:\n",
    "        conv = conv_templates[conv_mode].copy()\n",
    "        roles = conv.roles\n",
    "        inp = f\"{roles[0]}: {raw_prompt}\"\n",
    "        inp = (\n",
    "            DEFAULT_IM_START_TOKEN\n",
    "            + DEFAULT_IMAGE_TOKEN\n",
    "            + DEFAULT_IM_END_TOKEN\n",
    "            + \"\\n\"\n",
    "            + inp\n",
    "        )\n",
    "        conv.append_message(conv.roles[0], inp)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "    disable_torch_init()\n",
    "    image_tensor = (\n",
    "        image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        .half()\n",
    "        .cuda()\n",
    "    )\n",
    "\n",
    "    keywords = [stop_str]\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cuda()\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "        )\n",
    "    outputs = tokenizer.decode(\n",
    "        output_ids[0, input_ids.shape[1] :], skip_special_tokens=True\n",
    "    ).strip()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lora-model/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile lora-model/code/requirements.txt\n",
    "llava @ git+https://github.com/haotian-liu/LLaVA@v1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./special_tokens_map.json\n",
      "./generation_config.json\n",
      "./code/\n",
      "./code/requirements.txt\n",
      "./code/inference.py\n",
      "./config.json\n",
      "./training_args.bin\n",
      "./tokenizer_config.json\n",
      "./adapter_model/\n",
      "./adapter_model/adapter_model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./adapter_model/non_lora_trainables.bin\n",
      "./adapter_model/config.json\n",
      "./adapter_model/README.md\n",
      "./adapter_model/adapter_config.json\n",
      "./adapter_model/trainer_state.json\n",
      "./tokenizer.model\n",
      "./model.safetensors.index.json\n",
      "./model-00003-of-00003.safetensors\n",
      "./model-00002-of-00003.safetensors\n",
      "./trainer_state.json\n",
      "./model-00001-of-00003.safetensors\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf llava-lora.tar.gz --use-compress-program=pigz -C lora-model .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./llava-lora.tar.gz to s3://llava-ue1/llava-lora.tar.gz     \n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp llava-lora.tar.gz s3://llava-ue1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://llava-ue1/llava-lora.tar.gz\",      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.28.1\",  # transformers version used\n",
    "   pytorch_version=\"2.0.0\",       # pytorch version used\n",
    "   py_version='py310',            # python version used\n",
    "   model_server_workers=1\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    container_startup_health_check_timeout=600, # increase timeout for large models\n",
    "    model_data_download_timeout=600, # increase timeout for large models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"image\" : 'https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png', \n",
    "    \"question\" : \"Describe the image and color details.\",\n",
    "    # \"max_new_tokens\" : 1024,\n",
    "    # \"temperature\" : 0.2,\n",
    "    # \"stop_str\" : \"###\",\n",
    "}\n",
    "\n",
    "# request\n",
    "output = predictor.predict(data)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
