{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Lora fined tuned LLaVA on SageMaker\n",
    "As the time of writing, LLaVA is not yet fully integrated into HuggingFace's transformers library, it has to be loaded it from disk. We will create a tar.gz file containing the model weights and the inference code.\n",
    "Reference: https://medium.com/@liltom.eth/deploy-llava-1-5-on-amazon-sagemaker-168b2efd2489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a folder structure required by SageMaker and download the model weights and adapter weights from S3 (if you're using the weights from Hugging Face, you can use snapshot_download https://huggingface.co/docs/huggingface_hub/guides/download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p lora-model/code\n",
    "!mkdir -p lora-model/adapter_model\n",
    "\n",
    "!aws s3 cp s3://YOUR_BUCKET/FULL_TUNE_JOB_ID/checkpoints/ lora-model/ --recursive\n",
    "!aws s3 cp s3://YOUR_BUCKET/LORA_JOB_ID/checkpoints/ lora-model/adapter_model --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create infernece code and requirements\n",
    "Reference: https://github.com/haotian-liu/LLaVA/blob/main/llava/model/builder.py#L57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lora-model/code/inference.py\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from llava.model import LlavaLlamaForCausalLM\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, KeywordsStoppingCriteria\n",
    "\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    kwargs = {\"device_map\": \"auto\"}\n",
    "    kwargs[\"torch_dtype\"] = torch.float16\n",
    "    adapter_path = os.path.join(model_dir, \"adapter_model\")\n",
    "    \n",
    "    print(\"loading lora config\")\n",
    "    from llava.model.language_model.llava_llama import LlavaConfig\n",
    "    lora_cfg_pretrained = LlavaConfig.from_pretrained(adapter_path)\n",
    "    print(\"loading model\")\n",
    "    model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "        model_dir, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs\n",
    "    )\n",
    "    token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n",
    "    if model.lm_head.weight.shape[0] != token_num:\n",
    "        model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n",
    "        model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n",
    "\n",
    "    if os.path.exists(os.path.join(adapter_path, 'non_lora_trainables.bin')):\n",
    "        non_lora_trainables = torch.load(os.path.join(adapter_path, 'non_lora_trainables.bin'), map_location='cpu')\n",
    "\n",
    "    non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "    if any(k.startswith('model.model.') for k in non_lora_trainables):\n",
    "        non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "    model.load_state_dict(non_lora_trainables, strict=False)\n",
    "\n",
    "    from peft import PeftModel\n",
    "    print('Loading LoRA weights...')\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    print('Merging LoRA weights...')\n",
    "    model = model.merge_and_unload()\n",
    "    print('Model is loaded...')\n",
    "\n",
    "    print(\"loading tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
    "\n",
    "    vision_tower = model.get_vision_tower()\n",
    "    if not vision_tower.is_loaded:\n",
    "        vision_tower.load_model()\n",
    "    vision_tower.to(device=\"cuda\", dtype=torch.float16)\n",
    "    image_processor = vision_tower.image_processor\n",
    "    return model, tokenizer, image_processor\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    # unpack model and tokenizer\n",
    "    model, tokenizer, image_processor = model_and_tokenizer\n",
    "\n",
    "    # get prompt & parameters\n",
    "    image_file = data.pop(\"image\", data)\n",
    "    raw_prompt = data.pop(\"question\", data)\n",
    "\n",
    "    max_new_tokens = data.pop(\"max_new_tokens\", 1024)\n",
    "    temperature = data.pop(\"temperature\", 0.2)\n",
    "    conv_mode = data.pop(\"conv_mode\", \"llava_v1\")\n",
    "\n",
    "    if conv_mode == \"raw\":\n",
    "        # use raw_prompt as prompt\n",
    "        prompt = raw_prompt\n",
    "        stop_str = \"###\"\n",
    "    else:\n",
    "        conv = conv_templates[conv_mode].copy()\n",
    "        roles = conv.roles\n",
    "        inp = f\"{roles[0]}: {raw_prompt}\"\n",
    "        inp = (\n",
    "            DEFAULT_IM_START_TOKEN\n",
    "            + DEFAULT_IMAGE_TOKEN\n",
    "            + DEFAULT_IM_END_TOKEN\n",
    "            + \"\\n\"\n",
    "            + inp\n",
    "        )\n",
    "        conv.append_message(conv.roles[0], inp)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "    disable_torch_init()\n",
    "    image_tensor = (\n",
    "        image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        .half()\n",
    "        .cuda()\n",
    "    )\n",
    "\n",
    "    keywords = [stop_str]\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cuda()\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "        )\n",
    "    outputs = tokenizer.decode(\n",
    "        output_ids[0, input_ids.shape[1] :], skip_special_tokens=True\n",
    "    ).strip()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lora-model/code/requirements.txt\n",
    "llava @ git+https://github.com/haotian-liu/LLaVA@v1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tar and gzip the model and inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -cvf llava-lora.tar.gz --use-compress-program=pigz -C lora-model ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp llava-lora.tar.gz s3://YOUR_BUCKET/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create SageMaker inference endpoint from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://YOUR_BUCKET/llava-lora.tar.gz\",      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.28.1\",  # transformers version used\n",
    "   pytorch_version=\"2.0.0\",       # pytorch version used\n",
    "   py_version='py310',            # python version used\n",
    "   model_server_workers=1\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    container_startup_health_check_timeout=600, # increase timeout for large models\n",
    "    model_data_download_timeout=600, # increase timeout for large models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Wait until model is fully deployed and test it.\n",
    "The first invocation might time out as it lazy loads the model during the first call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"image\" : 'https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png', \n",
    "    \"question\" : \"Describe the image and color details.\",\n",
    "    # \"max_new_tokens\" : 1024,\n",
    "    # \"temperature\" : 0.2,\n",
    "    # \"stop_str\" : \"###\",\n",
    "}\n",
    "\n",
    "# the first call might time out as it lazy loads the model during the first call\n",
    "output = predictor.predict(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. If the model and endpoint are no longer needed, you can delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model() \n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
